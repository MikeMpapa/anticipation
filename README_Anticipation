# Running Anticipation transformer on Levanter

## Setting-up Anticipatory Transformer

1. Clone modified repo
```bash
git clone https://github.com/MikeMpapa/anticipation.git
```

Original repo: https://github.com/jthickstun/anticipation

2. Run in repository root:
```
pip install .
```

3. Install software dependencies
```
pip install -r requirements.txt
```

## Prepare the data

1. Preprocess the binary MIDI files to an intermediate text representation. 
```
python midi-preprocess.py $DATAPATH/lmd_full
```
2. Tokenize batches of intermediate LakhMIDI data files according to the vocabulary defined in `src/settings/vocab.py`. The top-level script depends upon the directory structure of the LakhMIDI dataset. Parallelism is again controlled by `PREPROC_WORKERS`. Choose a dataset augmentation factor (multiple of 10) for training an anticipatory infilling model, or 1 (default) for standard autoregressive training. Use the optional `-i` flag to generate training data for an interarrival-time model.

```
python tokenize-lakh.py $DATAPATH/lmd_full --augment 1
```

Define the train/validation/test splits.
```
mv $DATAPATH/$DATASET_FOLDER/tokenized-events-FOLDER_VALID.txt $DATAPATH/valid.txt
mv $DATAPATH/$DATASET_FOLDER/tokenized-events-FOLDER_TEST.txt $DATAPATH/test.txt
cat $DATAPATH/$DATASET_FOLDER/tokenized-events-*.txt > $DATAPATH/train-ordered.txt
```

Finally, we must shuffle our training data. For this shuffle procedure, your have a machine with enough RAM to read the whole training data file.
```
shuf $DATAPATH/$DATASET_FOLDER/train-ordered.txt > $DATAPATH/train.txt
```

The final preprocessed train/valid/test splits are available at `DATAPATH`.

### Resource Management

**Compute**. Preprocessing scripts are designed to run with multiprocessing parallelism: you can configure the number of workers using `PREPROC_WORKERS` in `src/settings/constants.py`.

**Memory**. The most memory intensive operation is the final shuffle operation, which requires the entire final training dataset to be loaded into memory. Alternative memory-efficient solutions do exist for shuffling (or approximately shuffling) the lines of a file, which you may wish to explore if memory is a constraint. Warning: we have observed that approximate shuffling using a *local* shuffle of the training data is not sufficient to achieve good model performance and should be avoided.

**Disk**. The base preprocessed LakhMidi dataset--prepared for autoregressive modeling without anticipation--is about 10Gb. The size of the dataset when prepared for anticipatory autoregressive modeling is a multiple of the base dataset size, controlled by `augment` parameter to `scripts/tokenize-lakh.py`. See the [Cleanup](###cleanup) for cleaning up temporary files on disk after preprocessing.

### Cleanup

Each stage of preprocessing generates temporary files with intermediate outputs. After completing preprocessing, these temporary files can be cleaned up as follows.

Delete the intermediate data files generated by the `midi-preprocess` script.
```
rm $DATAPATH/lmd_full/*/*.txt
```

Delete the intermediate tokenized events generated by the `tokenize-lakh` script.
```
rm $DATAPATH/lmd_full/tokenized-events-*.txt
```

Delete the ordered training data file, which is superseded by the shuffled version.
```
rm $DATAPATH/lmd_full/train-ordered.txt
```

## Fine-tune the model

1. Create a config file for the model and save in 'levanter/config'

Example: 
```
data:
  train_urls:
      - "data/SA/202407_midi_test_files/train.txt"
  validation_urls:
      - "data/SA/202407_midi_test_files/valid.txt"
  cache_dir: "/cache/"
  tokenizer: "passthrough"
  plaintext: True
  vocab_size: 55028
  enforce_eos: False
model:
  type: gpt2
  hidden_dim: 1024
  num_heads: 16
  num_layers: 24
  seq_len: 1024
  scale_attn_by_inverse_layer_idx: true
  embed_pdrop: 0.1
  resid_pdrop: 0.1
  gradient_checkpointing: true
initialize_from_hf: stanford-crfm/music-medium-800k
trainer:
  mp: p=f32,c=bfloat16
  model_axis_size: 1
  per_device_parallelism: 16
  num_train_steps: 30
  train_batch_size: 512
  per_device_eval_parallelism: 1

  checkpointer:
    base_path: /levander/checkpoints
    save_interval: 30m
    keep:
      - every: 1000

  axis_resources:
    batch: "data"
    vocab: "model"
    mlp: "model"
    heads: "model"
  parameter_axis_resources:
    embed: "data"
optimizer:
  learning_rate: 3E-5
  weight_decay: 0.1
  min_lr_ratio: 0.1
  ```

3. Set up the exporting config to be used after training to convert the model from Levanter to HuggingFace format

Example: 
```
model:
  type: gpt2
  hidden_dim: 1024
  num_heads: 16
  num_layers: 24
  seq_len: 1024
  scale_attn_by_inverse_layer_idx: true
  gradient_checkpointing: true
  use_flash_attention: false

override_vocab_size: 55028
output_dir: /checkpoints/your-checkpoint/hf
checkpoint_path: /checkpoints/your-checkpoint
save_tokenizer: false
```

4. Store data files under 'levanter/data' and configs under levanter/config
```
levanter/
├── data/
│   ├── train.txt
│   └── valid.txt
│   └── test.txt
├── config/
│   ├── MyConfigTrain.txt
    ├── MyConfigExport.txt
```

5. Setting up Levanter (General Info)

### Mounting a Local Fork of Levanter Inside the Docker Container

If you are planning to add to or extend Levanter for your own use case, follow these Docker setup steps.

First, clone the Levanter repository:


```bash
git clone https://github.com/MikeMpapa/levanter.git
```
Original repo: https://github.com/stanford-crfm/levanter.git


Then run an interactive Docker container with your Levanter directory mounted as a volume. For example, if your Levanter
repo is located at `/nlp/src/username/levanter`, then run the command below to make that directory accessible to the Docker container.

```bash
sudo docker run -it --gpus=all -v /nlp/src/username/levanter:/levanter --shm-size=16g ghcr.io/nvidia/jax:levanter
```

Once your container starts, the Levanter repo you cloned will be available at `/levanter`.
You should `cd` into the `levanter` directory and run the install command for Levanter from that directory.

```bash
cd /levanter
pip install -e .
```

Now, you should be able to run training jobs in this container using the version of Levanter from your mounted directory:

```bash
python src/levanter/main/train_lm.py \
    --config_path config/gpt2_small.yaml
```

### Multi-Node GPU Training
For multi-gpu training, you need to additionally have [nvidia-fabricmanager](https://docs.nvidia.com/datacenter/tesla/pdf/fabric-manager-user-guide.pdf) installed on each of your nodes.

```
sudo apt-get install cuda-drivers-fabricmanager
sudo systemctl start nvidia-fabricmanager
```

#### Multi-Node Docker Environment Setup
If you are using a docker container to train your model, your docker run command should look similar to this

```
sudo docker run -it --network=host -v ~/src/levanter/cache:/cache -v /home/user/levanter:/levanter --gpus=all --shm-size=16g  ghcr.io/nvidia/jax:levanter
```
The main difference between the command here and the one found in the [GPU Docker Development Guide](dev/GPU-Docker-Dev.md) is the `--network=host` argument. This tells the docker container to use the host machine's network instead of the default docker `bridge` network. Using `host` is the easiest way to do multi-node networking with docker and should be sufficient for your training purposes. Please see docker's [host](https://docs.docker.com/network/network-tutorial-host/) and [bridge](https://docs.docker.com/network/network-tutorial-standalone/) network documentation for more information.

#### Multi-Node Training Command
We use [JAX Distributed](https://jax.readthedocs.io/en/latest/multi_process.html) to help manage multi-node training in Levanter. On each node you can run a command like the following to kick off a training job:
```
NCCL_DEBUG=INFO python src/levanter/main/train_lm.py \
  --config_path config/gpt2_7b.yaml \
  --trainer.ray.auto_start_cluster false \
  --trainer.per_device_parallelism -1 \
  --trainer.distributed.num_processes 4 \
  --trainer.distributed.local_device_ids "[0,1,2,3,4,5,6,7]" \
  --trainer.distributed.coordinator_address 12.345.678.91:2403 \
  --trainer.distributed.process_id 0
```
This will start a 4 node job where each node has 8 GPUs.

- `--trainer.distributed.num_processes` - sets the number of nodes used in this training run
- `--trainer.distributed.local_device_ids` - sets the ids of the local GPUs to use on this specific node
- `--trainer.distributed.coordinator_address` - is the IP address and port number of the node that will be leading the training run. All other nodes should have network access to the port and IP address set by this argument. The same IP address and port number should be used for this argument in every node's run command.
- `--trainer.distributed.process_id` - The process ID of the current node. If the node is coordinator for the training run (its IP address was the one specified at `--trainer.distributed.coordinator_address`), its process ID needs to be set to zero. All other nodes in the train run should have a unique integer ID between [1, `num_processes` - 1].

When the above command is run on the coordinator node, it will block until all other processes connect to it. All the other nodes will connect to the coordinator node before they can begin training. All other training run arguments have the same meaning as with single node runs. We recommend thinking about increasing your `--trainer.train_batch_size` value when you scale from single node to multi-node training, as this is the global batch size for your training job and you've now increased your compute capacity.


6. Mount customized levanter with anticipation data and configs to the levanter-container 
```bash
sudo docker run -it --gpus=all -v /nlp/src/username/levanter:/levanter --shm-size=16g ghcr.io/nvidia/jax:levanter
```

7. Move to the container and run:
```bash
cd /levanter
pip install -e .
```

8. Install torch
```bash
pip install torch .
```

9. Train the model:
```bash
python -m levanter.main.train_lm --config_path config/finetune.yaml
```

10. Export the model to HF format
```bash
python -m levanter.main.export_lm_to_hf.py --config_path config/lakh_medium_export.yaml
```
